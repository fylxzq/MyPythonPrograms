{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'local[*]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schour.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_df = spark.read.format(\"csv\").option(\"header\",True).option(\"delimiter\",\",\").load(\"/input/hour.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['instant',\n",
       " 'dteday',\n",
       " 'season',\n",
       " 'yr',\n",
       " 'mnth',\n",
       " 'hr',\n",
       " 'holiday',\n",
       " 'weekday',\n",
       " 'workingday',\n",
       " 'weathersit',\n",
       " 'temp',\n",
       " 'atemp',\n",
       " 'hum',\n",
       " 'windspeed',\n",
       " 'casual',\n",
       " 'registered',\n",
       " 'cnt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hour_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour_df = hour_df.drop(\"instant\").drop(\"dteday\").drop(\"yr\").drop(\"casual\").drop(\"registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: string (nullable = true)\n",
      " |-- mnth: string (nullable = true)\n",
      " |-- hr: string (nullable = true)\n",
      " |-- holiday: string (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- workingday: string (nullable = true)\n",
      " |-- weathersit: string (nullable = true)\n",
      " |-- temp: string (nullable = true)\n",
      " |-- atemp: string (nullable = true)\n",
      " |-- hum: string (nullable = true)\n",
      " |-- windspeed: string (nullable = true)\n",
      " |-- cnt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hour_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_df = hour_df.select([col(column).cast(\"double\").alias(column) for column in hour_df.columns] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('season', 'double'),\n",
       " ('mnth', 'double'),\n",
       " ('hr', 'double'),\n",
       " ('holiday', 'double'),\n",
       " ('weekday', 'double'),\n",
       " ('workingday', 'double'),\n",
       " ('weathersit', 'double'),\n",
       " ('temp', 'double'),\n",
       " ('atemp', 'double'),\n",
       " ('hum', 'double'),\n",
       " ('windspeed', 'double'),\n",
       " ('cnt', 'double')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hour_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+\n",
      "|season|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed| cnt|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+\n",
      "|   1.0| 1.0|0.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.81|      0.0|16.0|\n",
      "|   1.0| 1.0|1.0|    0.0|    6.0|       0.0|       1.0|0.22|0.2727| 0.8|      0.0|40.0|\n",
      "|   1.0| 1.0|2.0|    0.0|    6.0|       0.0|       1.0|0.22|0.2727| 0.8|      0.0|32.0|\n",
      "|   1.0| 1.0|3.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.75|      0.0|13.0|\n",
      "|   1.0| 1.0|4.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.75|      0.0| 1.0|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hour_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df,test_df = hour_df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[season: double, mnth: double, hr: double, holiday: double, weekday: double, workingday: double, weathersit: double, temp: double, atemp: double, hum: double, windspeed: double, cnt: double]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer,VectorAssembler,VectorIndexer\n",
    "from pyspark.ml.regression import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureCols = hour_df.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoAssember = VectorAssembler(inputCols=featureCols,outputCol=\"aFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorindexer = VectorIndexer(inputCol=\"aFeatures\",outputCol=\"features\",maxCategories=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(labelCol=\"cnt\",featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "dt_pipeline = Pipeline(stages=[vectoAssember,vectorindexer,dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_pipelineModel = dt_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_4cbc889394e538c8eeea) of depth 5 with 63 nodes"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_pipelineModel.stages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_df = dt_pipelineModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['season',\n",
       " 'mnth',\n",
       " 'hr',\n",
       " 'holiday',\n",
       " 'weekday',\n",
       " 'workingday',\n",
       " 'weathersit',\n",
       " 'temp',\n",
       " 'atemp',\n",
       " 'hum',\n",
       " 'windspeed',\n",
       " 'cnt',\n",
       " 'aFeatures',\n",
       " 'features',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "| cnt|        prediction|\n",
      "+----+------------------+\n",
      "|59.0| 59.30487804878049|\n",
      "|33.0| 59.30487804878049|\n",
      "|91.0| 59.30487804878049|\n",
      "| 7.0|37.438395415472776|\n",
      "|12.0|37.438395415472776|\n",
      "+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_df.select(\"cnt\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evalutor = RegressionEvaluator(labelCol=\"cnt\",predictionCol=\"prediction\",metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse = evalutor.evaluate(predicted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.28882070604166"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder,TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    ".addGrid(dt.maxDepth,[5,10,15,25])\\\n",
    ".addGrid(dt.maxBins,[25,35,45,50])\\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator=dt,evaluator=evalutor,estimatorParamMaps=paramGrid,trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvs_pipeline = Pipeline(stages=[vectoAssember,vectorindexer,tvs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs_pipelineModel = tvs_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.14427234919033"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = tvs_pipelineModel.transform(test_df)\n",
    "rmse = evalutor.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_4cbc889394e538c8eeea) of depth 10 with 1743 nodes"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel = tvs_pipelineModel.stages[2].bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.regression.DecisionTreeRegressionModel"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=dt,evaluator=evalutor,estimatorParamMaps=paramGrid,numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_pipeline = Pipeline(stages=[vectoAssember,vectorindexer,cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_pipelineModel = cv_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = cv_pipelineModel.transform(test_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse = evalutor.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.72006422153424"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(labelCol=\"cnt\",featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbt_pipeline = Pipeline(stages=[vectoAssember,vectorindexer,gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parmGrid = ParamGridBuilder().addGrid(gbt.maxDepth,[5,10]).addGrid(gbt.maxBins,[25,40]).addGrid(gbt.maxIter,[10,50]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=gbt,evaluator=evalutor,estimatorParamMaps=paramGrid,numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_pipeline = Pipeline(stages=[vectoAssember,vectorindexer,cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_pipelineModel = cv_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = cv_pipelineModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse = evalutor.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.25497291952546"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
