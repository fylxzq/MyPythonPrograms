{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'local[*]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawData = sc.textFile(\"/input/covtype.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = rawData.map(lambda x:x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "581012"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fildnum = len(lines.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StringType,StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filds = [StructField(\"f\" + str(i),StringType(),True) for i in range(fildnum)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType(filds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covtype_df = spark.createDataFrame(lines,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- f0: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      " |-- f4: string (nullable = true)\n",
      " |-- f5: string (nullable = true)\n",
      " |-- f6: string (nullable = true)\n",
      " |-- f7: string (nullable = true)\n",
      " |-- f8: string (nullable = true)\n",
      " |-- f9: string (nullable = true)\n",
      " |-- f10: string (nullable = true)\n",
      " |-- f11: string (nullable = true)\n",
      " |-- f12: string (nullable = true)\n",
      " |-- f13: string (nullable = true)\n",
      " |-- f14: string (nullable = true)\n",
      " |-- f15: string (nullable = true)\n",
      " |-- f16: string (nullable = true)\n",
      " |-- f17: string (nullable = true)\n",
      " |-- f18: string (nullable = true)\n",
      " |-- f19: string (nullable = true)\n",
      " |-- f20: string (nullable = true)\n",
      " |-- f21: string (nullable = true)\n",
      " |-- f22: string (nullable = true)\n",
      " |-- f23: string (nullable = true)\n",
      " |-- f24: string (nullable = true)\n",
      " |-- f25: string (nullable = true)\n",
      " |-- f26: string (nullable = true)\n",
      " |-- f27: string (nullable = true)\n",
      " |-- f28: string (nullable = true)\n",
      " |-- f29: string (nullable = true)\n",
      " |-- f30: string (nullable = true)\n",
      " |-- f31: string (nullable = true)\n",
      " |-- f32: string (nullable = true)\n",
      " |-- f33: string (nullable = true)\n",
      " |-- f34: string (nullable = true)\n",
      " |-- f35: string (nullable = true)\n",
      " |-- f36: string (nullable = true)\n",
      " |-- f37: string (nullable = true)\n",
      " |-- f38: string (nullable = true)\n",
      " |-- f39: string (nullable = true)\n",
      " |-- f40: string (nullable = true)\n",
      " |-- f41: string (nullable = true)\n",
      " |-- f42: string (nullable = true)\n",
      " |-- f43: string (nullable = true)\n",
      " |-- f44: string (nullable = true)\n",
      " |-- f45: string (nullable = true)\n",
      " |-- f46: string (nullable = true)\n",
      " |-- f47: string (nullable = true)\n",
      " |-- f48: string (nullable = true)\n",
      " |-- f49: string (nullable = true)\n",
      " |-- f50: string (nullable = true)\n",
      " |-- f51: string (nullable = true)\n",
      " |-- f52: string (nullable = true)\n",
      " |-- f53: string (nullable = true)\n",
      " |-- f54: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covtype_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = covtype_df.select([col(column).cast(\"double\").alias(column) for column in covtype_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- f0: double (nullable = true)\n",
      " |-- f1: double (nullable = true)\n",
      " |-- f2: double (nullable = true)\n",
      " |-- f3: double (nullable = true)\n",
      " |-- f4: double (nullable = true)\n",
      " |-- f5: double (nullable = true)\n",
      " |-- f6: double (nullable = true)\n",
      " |-- f7: double (nullable = true)\n",
      " |-- f8: double (nullable = true)\n",
      " |-- f9: double (nullable = true)\n",
      " |-- f10: double (nullable = true)\n",
      " |-- f11: double (nullable = true)\n",
      " |-- f12: double (nullable = true)\n",
      " |-- f13: double (nullable = true)\n",
      " |-- f14: double (nullable = true)\n",
      " |-- f15: double (nullable = true)\n",
      " |-- f16: double (nullable = true)\n",
      " |-- f17: double (nullable = true)\n",
      " |-- f18: double (nullable = true)\n",
      " |-- f19: double (nullable = true)\n",
      " |-- f20: double (nullable = true)\n",
      " |-- f21: double (nullable = true)\n",
      " |-- f22: double (nullable = true)\n",
      " |-- f23: double (nullable = true)\n",
      " |-- f24: double (nullable = true)\n",
      " |-- f25: double (nullable = true)\n",
      " |-- f26: double (nullable = true)\n",
      " |-- f27: double (nullable = true)\n",
      " |-- f28: double (nullable = true)\n",
      " |-- f29: double (nullable = true)\n",
      " |-- f30: double (nullable = true)\n",
      " |-- f31: double (nullable = true)\n",
      " |-- f32: double (nullable = true)\n",
      " |-- f33: double (nullable = true)\n",
      " |-- f34: double (nullable = true)\n",
      " |-- f35: double (nullable = true)\n",
      " |-- f36: double (nullable = true)\n",
      " |-- f37: double (nullable = true)\n",
      " |-- f38: double (nullable = true)\n",
      " |-- f39: double (nullable = true)\n",
      " |-- f40: double (nullable = true)\n",
      " |-- f41: double (nullable = true)\n",
      " |-- f42: double (nullable = true)\n",
      " |-- f43: double (nullable = true)\n",
      " |-- f44: double (nullable = true)\n",
      " |-- f45: double (nullable = true)\n",
      " |-- f46: double (nullable = true)\n",
      " |-- f47: double (nullable = true)\n",
      " |-- f48: double (nullable = true)\n",
      " |-- f49: double (nullable = true)\n",
      " |-- f50: double (nullable = true)\n",
      " |-- f51: double (nullable = true)\n",
      " |-- f52: double (nullable = true)\n",
      " |-- f53: double (nullable = true)\n",
      " |-- f54: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fearturesCols = df.columns[:54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"label\",df.f54 - 1).drop(\"f54\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+-----+---+-----+-----+-----+-----+------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+\n",
      "|    f0|  f1| f2|   f3| f4|   f5|   f6|   f7|   f8|    f9|f10|f11|f12|f13|f14|f15|f16|f17|f18|f19|f20|f21|f22|f23|f24|f25|f26|f27|f28|f29|f30|f31|f32|f33|f34|f35|f36|f37|f38|f39|f40|f41|f42|f43|f44|f45|f46|f47|f48|f49|f50|f51|f52|f53|label|\n",
      "+------+----+---+-----+---+-----+-----+-----+-----+------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+\n",
      "|2596.0|51.0|3.0|258.0|0.0|510.0|221.0|232.0|148.0|6279.0|1.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|1.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|  4.0|\n",
      "+------+----+---+-----+---+-----+-----+-----+-----+------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df = df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[f0: double, f1: double, f2: double, f3: double, f4: double, f5: double, f6: double, f7: double, f8: double, f9: double, f10: double, f11: double, f12: double, f13: double, f14: double, f15: double, f16: double, f17: double, f18: double, f19: double, f20: double, f21: double, f22: double, f23: double, f24: double, f25: double, f26: double, f27: double, f28: double, f29: double, f30: double, f31: double, f32: double, f33: double, f34: double, f35: double, f36: double, f37: double, f38: double, f39: double, f40: double, f41: double, f42: double, f43: double, f44: double, f45: double, f46: double, f47: double, f48: double, f49: double, f50: double, f51: double, f52: double, f53: double, label: double]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=fearturesCols,outputCol=\"features\")\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\",featuresCol=\"features\",maxDepth=5,maxBins=20)\n",
    "dt_pipeline = Pipeline(stages=[vectorAssembler,dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = dt_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = pipelineModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(f0=1863.0, f1=37.0, f2=17.0, f3=120.0, f4=18.0, f5=90.0, f6=217.0, f7=202.0, f8=115.0, f9=769.0, f10=0.0, f11=0.0, f12=0.0, f13=1.0, f14=0.0, f15=1.0, f16=0.0, f17=0.0, f18=0.0, f19=0.0, f20=0.0, f21=0.0, f22=0.0, f23=0.0, f24=0.0, f25=0.0, f26=0.0, f27=0.0, f28=0.0, f29=0.0, f30=0.0, f31=0.0, f32=0.0, f33=0.0, f34=0.0, f35=0.0, f36=0.0, f37=0.0, f38=0.0, f39=0.0, f40=0.0, f41=0.0, f42=0.0, f43=0.0, f44=0.0, f45=0.0, f46=0.0, f47=0.0, f48=0.0, f49=0.0, f50=0.0, f51=0.0, f52=0.0, f53=0.0, label=5.0, features=SparseVector(54, {0: 1863.0, 1: 37.0, 2: 17.0, 3: 120.0, 4: 18.0, 5: 90.0, 6: 217.0, 7: 202.0, 8: 115.0, 9: 769.0, 13: 1.0, 15: 1.0}), rawPrediction=DenseVector([0.0, 422.0, 11821.0, 1159.0, 0.0, 4875.0, 0.0]), probability=DenseVector([0.0, 0.0231, 0.6468, 0.0634, 0.0, 0.2667, 0.0]), prediction=2.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----+----------+\n",
      "|            features|       rawPrediction|         probability|label|prediction|\n",
      "+--------------------+--------------------+--------------------+-----+----------+\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,422.0,11821....|[0.0,0.0230891284...|  5.0|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,422.0,11821....|[0.0,0.0230891284...|  5.0|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,422.0,11821....|[0.0,0.0230891284...|  5.0|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,422.0,11821....|[0.0,0.0230891284...|  5.0|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,422.0,11821....|[0.0,0.0230891284...|  5.0|       2.0|\n",
      "|(54,[0,1,2,5,6,7,...|[0.0,45.0,442.0,7...|[0.0,0.0306748466...|  5.0|       3.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,422.0,11821....|[0.0,0.0230891284...|  5.0|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,422.0,11821....|[0.0,0.0230891284...|  2.0|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,422.0,11821....|[0.0,0.0230891284...|  5.0|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,422.0,11821....|[0.0,0.0230891284...|  2.0|       2.0|\n",
      "+--------------------+--------------------+--------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted.select(\"features\",\"rawPrediction\",\"probability\",\"label\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import  MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = pipelineModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7017378595325363"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder,TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parmGrid = ParamGridBuilder()\\\n",
    ".addGrid(dt.impurity,[\"gini\",\"entropy\"])\\\n",
    ".addGrid(dt.maxDepth,[10,15,25])\\\n",
    ".addGrid(dt.maxBins,[30,40,50]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator=dt,evaluator=evaluator,estimatorParamMaps=parmGrid,trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvs_pipeline = Pipeline(stages=[vectorAssembler,tvs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvs_pipelineModel  = tvs_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
