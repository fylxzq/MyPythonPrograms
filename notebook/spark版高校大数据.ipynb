{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高校大数据，扑克牌组合优化等级标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 训练数据集包括11个字段，字段的含义如下所示，牌面花色用C,D,H,S表示，分表代表梅花、方块、红桃和黑桃，牌面大小用1-10以及J、Q、K来表示。需要注意，字段11是每条数据的优化等级标注。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+----+\n",
      "|_c0|_c1|_c2|_c3|_c4|_c5|_c6|_c7|_c8|_c9|_c10|\n",
      "+---+---+---+---+---+---+---+---+---+---+----+\n",
      "|  H| 10|  H|  J|  H|  Q|  H|  K|  H|  1|   9|\n",
      "|  S|  J|  S|  K|  S|  Q|  S| 10|  S|  1|   9|\n",
      "|  D|  Q|  D|  J|  D| 10|  D|  K|  D|  1|   9|\n",
      "|  C| 10|  C|  J|  C|  K|  C|  1|  C|  Q|   9|\n",
      "|  C|  1|  C|  K|  C|  J|  C|  Q|  C| 10|   9|\n",
      "|  H|  2|  H|  4|  H|  3|  H|  5|  H|  6|   8|\n",
      "|  H|  9|  H|  Q|  H|  J|  H| 10|  H|  K|   8|\n",
      "|  S|  1|  S|  2|  S|  4|  S|  3|  S|  5|   8|\n",
      "|  D|  5|  D|  6|  D|  7|  D|  9|  D|  8|   8|\n",
      "|  C|  1|  C|  4|  C|  3|  C|  2|  C|  5|   8|\n",
      "+---+---+---+---+---+---+---+---+---+---+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('training.csv')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+----+\n",
      "|_c0|_c1|_c2|_c3|_c4|_c5|_c6|_c7|_c8|_c9|_c10|\n",
      "+---+---+---+---+---+---+---+---+---+---+----+\n",
      "|  3| 10|  3| 11|  3| 12|  3| 13|  3| 14|   9|\n",
      "|  4| 11|  4| 13|  4| 12|  4| 10|  4| 14|   9|\n",
      "|  2| 12|  2| 11|  2| 10|  2| 13|  2| 14|   9|\n",
      "|  1| 10|  1| 11|  1| 13|  1| 14|  1| 12|   9|\n",
      "|  1| 14|  1| 13|  1| 11|  1| 12|  1| 10|   9|\n",
      "|  3|  2|  3|  4|  3|  3|  3|  5|  3|  6|   8|\n",
      "|  3|  9|  3| 12|  3| 11|  3| 10|  3| 13|   8|\n",
      "|  4| 14|  4|  2|  4|  4|  4|  3|  4|  5|   8|\n",
      "|  2|  5|  2|  6|  2|  7|  2|  9|  2|  8|   8|\n",
      "|  1| 14|  1|  4|  1|  3|  1|  2|  1|  5|   8|\n",
      "+---+---+---+---+---+---+---+---+---+---+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#为保证数据都是数值型，将字符做转换\n",
    "dict0 = {'1':'A'} \n",
    "df = df.replace(dict0)\n",
    "dict1 = {'H':'3','S':'4','D':'2','C':'1','J':'11','Q':'12','K':'13','A':'14'} \n",
    "df = df.replace(dict1)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas = df.select(df._c0.cast('int'), df._c1.cast('int'), df._c2.cast('int'), df._c3.cast('int'),\n",
    "                 df._c4.cast('int'),df._c5.cast('int'),df._c6.cast('int'),df._c7.cast('int'),\n",
    "                 df._c8.cast('int'),df._c9.cast('int'),df._c10.cast('int'))\n",
    "\n",
    "#通过select函数可以实现datas的列操作\n",
    "type(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#转化成pandas.dataframe.\n",
    "#pyspark.sql.dataframe.DataFrame --> pandas.core.dataFrame --> spark.createDateframe\n",
    "datas = datas.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#去重求数方法，如f([1,2,3,4,4]) = f([1,2,3,4]) = 4\n",
    "def notAlong(a):\n",
    "    l=[]\n",
    "    for i in set(a):\n",
    "        l.append(a.count(i))\n",
    "    return len(set(a)) - l.count(1)\n",
    "#重复出现最大次\n",
    "def maxrepeat(b):\n",
    "    n=[]\n",
    "    for i in set(b):\n",
    "        n.append(b.count(i))\n",
    "    return max(n)\n",
    "\n",
    "temp = datas\n",
    "#特征工程\n",
    "#处理花色\n",
    "temp1 = datas[['_c0', '_c2','_c4', '_c6', '_c8']]\n",
    "max_repeat_time = temp1.apply(lambda x: maxrepeat([i for i in np.array(x)]),axis = 1)\n",
    "#处理点数\n",
    "temp2 = datas[['_c1', '_c3','_c5', '_c7', '_c9']]\n",
    "temp2 = temp2.astype(float)\n",
    "#一组数的均值\n",
    "meanPoint = temp2.apply(np.mean, axis = 1)\n",
    "#一组数的方差\n",
    "varPoint = temp2.apply(np.var, axis = 1)\n",
    "setLastPoint = temp2.apply(lambda x: len(set([i for i in np.array(x)])),axis=1)\n",
    "notLPoint = temp2.apply(lambda x: notAlong([i for i  in np.array(x)]),axis=1)\n",
    "\n",
    "da = {'a':meanPoint, 'b':varPoint, 'c':setLastPoint, 'd':notLPoint,'e':max_repeat_time}\n",
    "frame = pd.DataFrame(da)\n",
    "frame['c10'] = temp['_c10']\n",
    "datas = spark.createDataFrame(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'c10']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|features              |label|\n",
      "+----------------------+-----+\n",
      "|[12.0,2.0,5.0,0.0,5.0]|9    |\n",
      "|[12.0,2.0,5.0,0.0,5.0]|9    |\n",
      "|[12.0,2.0,5.0,0.0,5.0]|9    |\n",
      "|[12.0,2.0,5.0,0.0,5.0]|9    |\n",
      "|[12.0,2.0,5.0,0.0,5.0]|9    |\n",
      "+----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "ins = datas.columns\n",
    "ins.remove('c10')\n",
    "assember = VectorAssembler(inputCols = ins,outputCol = 'features')\n",
    "output = assember.transform(datas)\n",
    "label_features = output.select('features','c10').toDF('features', 'label')\n",
    "label_features.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[3.6,1.8400000000...|    3|[0.0,0.0,0.0,8498...|[0.0,0.0,0.0,1.0,...|       3.0|\n",
      "|[3.8,3.7600000000...|    3|[0.0,0.0,0.0,8498...|[0.0,0.0,0.0,1.0,...|       3.0|\n",
      "|[3.8,4.96,3.0,1.0...|   14|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|      14.0|\n",
      "|[4.0,1.2,3.0,2.0,...|    2|[0.0,0.0,956.0,0....|[0.0,0.0,1.0,0.0,...|       2.0|\n",
      "|[4.0,2.8,4.0,1.0,...|    3|[0.0,0.0,0.0,8498...|[0.0,0.0,0.0,1.0,...|       3.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "(trainingData, testData) = label_features.randomSplit([0.8, 0.2])\n",
    "model = model.fit(trainingData)\n",
    "predictions=model.transform(testData)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996005592170961"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label',\n",
    "            metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
